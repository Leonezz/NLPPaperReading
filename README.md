<<<<<<< HEAD:lzl_notes/README.md
# Paper reading list for NLP research

## Transformer architecture

- **Attention is all you need**. *Ashish Vaswani*, *Noam Shazeer*, *Niki Parmar*, et al,. *In* NIPS 2017. [pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

## Pre-trained Models

- **BERT: pre-training of deep bidirectional transformers for language understanding**. *Jacob Devlin*, *Ming-Wei Chang*, *Kenton Lee*, *Kristina Toutanova*. *In* NAACL 2019. [pdf](https://aclanthology.org/N19-1423.pdf)
- **BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension**. *Mike Lewis*, *Yinhan Liu*, *Naman Goyal*, et al,. *In* ACL 2020. [pdf](https://aclanthology.org/2020.acl-main.703.pdf)
- **Improving language understanding by generative pre-training**. *Alec Radford*, *Tim Salimans*, *Ilya Sutskever*. *In* OpenAI blog 2018. [pdf](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)


## QA

=======
# Paper reading list for NLP research

## Zotero intro

[Zotero introduction](https://zotero-cn.github.io/e-zotero-md/)

## Transformer architecture

- Ashish Vaswani, Noam Shazeer, Niki Parmar, et al. **Attention is all you need**. In *Advances in neural information processing systems*, volume 30. Curran Associates, Inc. 2017. [[pdf](https://doi.org/10/gpnmtv)]

- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)

- [labml.ai Annotated PyTorch Paper Implementations](https://nn.labml.ai/index.html)

## Pre-trained Models

Refer to [this slide](./reference_note/A-Brief-Introduction-to-PLMs/slide/Introduction-to-PLM.pdf) for a brief introduction.

- Jacob Devlin, Ming-Wei Chang, Kenton Lee, et al. **BERT: pre-training of deep bidirectional transformers for language understanding**. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 4171–4186, Minneapolis, Minnesota. Association for Computational Linguistics. 2019. [[pdf](https://doi.org/10/ggbwf6)]

- Mike Lewis, Yinhan Liu, Naman Goyal, et al. **BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension**. In *Proceedings of the 58th annual meeting of the association for computational linguistics*, pages 7871–7880, Online. Association for Computational Linguistics. 2020. [[pdf](https://doi.org/10.18653/v1/2020.acl-main.703)]

- Alec Radford, Tim Salimans, and Ilya Sutskever. **Improving language understanding by generative pre-training**.In 2018. [[pdf](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)]

- Yinhan Liu, Myle Ott, Naman Goyal, et al. **RoBERTa: a robustly optimized BERT pretraining approach**. 2019. 00000 
arXiv: 1907.11692. [[pdf](http://arxiv.org/abs/1907.11692)]

- Zhilin Yang, Zihang Dai, Yiming Yang, et al. **Xlnet: generalized autoregressive pretraining for language understanding**. In *Advances in neural information processing systems*, volume 32. 2019. [[pdf](http://papers.nips.cc/paper/8812-xlnet-generalizedautoregressive-pretraining-for-language-understanding.pdf)]

- Zhengyan Zhang, Xu Han, Zhiyuan Liu, et al. **ERNIE: enhanced language representation with informative entities**.In *arXiv:1905.07129 [cs]*. 2019. arXiv: 1905.07129. [[pdf](http://arxiv.org/abs/1905.07129)]

- Yu Sun, Shuohuan Wang, Yukun Li, et al. **ERNIE: enhanced representation through knowledge integration**.In *arXiv:1904.09223 [cs]*. 2019. arXiv: 1904.09223. [[pdf](http://arxiv.org/abs/1904.09223)]


## QA

>>>>>>> 22c717ab3ce0af4db9d861700881c5d0b664cbf6:README.md
For **any** questions, feel free to open issues or contact [zhuwenq](https://github.com/Leonezz) directly.